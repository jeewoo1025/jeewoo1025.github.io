<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="ï¸-my-review">ğŸ’ğŸ»â€â™€ï¸ My Review</h2> <blockquote> <p>Paper: <a href="https://arxiv.org/abs/1706.03762v5" rel="external nofollow noopener" target="_blank">Attention Is All You Need</a></p> </blockquote> <p>In 2017, Google introduced the most powerful language model called <strong>Transformer</strong>. Transformer follows an encoder-decoder structure and uses residual-connected feedforward networks with attention layers in between. Itâ€™s defined by three core components as follows:</p> <ul> <li>Multi-head Attention</li> <li>Scaled Dot-Product Attention</li> <li>Positional Encoding</li> </ul> <p>âœ¨ What impressed me most was how Transformer achieved a strong understanding of a context using attention mechanisms. Itâ€™s no surprise that most of todayâ€™s SOTA language models are based on Transformers.</p> <p><br></p> <h2 id="-questions-about-transformer">ğŸ§  Questions About Transformer</h2> <h4 id="1-why-do-we-use-a-feedforward-network">1. Why do we use a Feedforward Network?</h4> <p>This is because expanding the dimensionality allows the model to perform richer computations. This helps uncover hidden patterns or features that may not be captured otherwise. Itâ€™s like giving the model more space to think.</p> <h4 id="2-why-is-it-called-a-position-wise-feedforward-network">2. Why is it called a Position-wise Feedforward Network?</h4> <p>When I looked into the code, it seemed pretty much like a regular feedforward network. ğŸ¤” So why call it â€œposition-wiseâ€?</p> <blockquote> <p>In my opinion, the authors probably wanted to emphasize that the operation is applied independently to each token (i.e., â€œpositionâ€) with its own set of weights. Even though the structure is the same, each token gets its own transformation individually.</p> </blockquote> <h4 id="3-what-are-residual-connections-and-layer-normalization-and-why-are-they-used">3. What are Residual Connections and Layer Normalization, and why are they used?</h4> <p>A residual connection adds the input <code class="language-plaintext highlighter-rouge">x</code> to the output of a layer <code class="language-plaintext highlighter-rouge">F(x)</code>, resulting in <code class="language-plaintext highlighter-rouge">x + F(x)</code>.</p> <p>â“ Why use it?</p> <blockquote> <p>If you take the derivative of <code class="language-plaintext highlighter-rouge">x + F(x)</code> with respect to <code class="language-plaintext highlighter-rouge">x</code>, you always get at least <code class="language-plaintext highlighter-rouge">1</code>. This helps mitigate the vanishing gradient problem, especially in deep neural networks. <br></p> </blockquote> <p>â“ Why use it?</p> <blockquote> <p>Even if inputs are normalized at the input layer, as they pass through hidden layers, their distribution tends to shift. This can push activations into saturation zones (where gradients vanish). Layer normalization helps maintain a balanced distribution for effective backpropagation. ğŸ’ª</p> </blockquote> <h4 id="4-what-exactly-is-positional-encoding">4. What exactly is Positional Encoding?</h4> <p>Since Transformers donâ€™t process tokens sequentially like RNNs, they need some ways to understand token order. Thatâ€™s where positional encoding comes in.</p> <p>It generates a unique position vector of shape <code class="language-plaintext highlighter-rouge">(1, d_model)</code> for each token using sine and cosine functions.</p> <p>â“ Why is it important?</p> <blockquote> <p>RNNs naturally process words in order, so they â€œknowâ€ where each word appears. However, Transformers process all tokens in parallel. So, positional encodings inject information about word order into the input representations.</p> </blockquote> <h4 id="5-why-do-we-use-the-attention-mechanism">5. Why do we use the Attention Mechanism?</h4> <p>Self-attention compares a token to all others in the input context to find the most relevant ones using dot products. This allows each token to â€œfocusâ€ on parts of the context that are semantically important to it.</p> <p>â“ Why is this valuable?</p> <blockquote> <p>This is because we can understand which parts of the input a token attends to most. As we go deeper into the encoder layers, tokens generally develop stronger attentions to contextually important words.</p> </blockquote> <p>â“ What are semantically similar tokens?</p> <blockquote> <p>Words that have related meanings and tend to appear in similar contexts. In vector space, they lie close to each other.</p> </blockquote> <p><br></p> <h2 id="-recommended-pytorch-transformer-codebases">ğŸ§¾ Recommended PyTorch Transformer Codebases</h2> <ul> <li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" rel="external nofollow noopener" target="_blank">PyTorch Official Tutorial</a></li> <li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="external nofollow noopener" target="_blank">The Annotated Transformer by Harvard NLP</a></li> <li><a href="https://github.com/paul-hyun/transformer-evolution" rel="external nofollow noopener" target="_blank">paul-hyun/transformer-evolution</a></li> <li><a href="https://github.com/tunz/transformer-pytorch" rel="external nofollow noopener" target="_blank">tunz/transformer-pytorch</a></li> </ul> </body></html>