<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jeewoo1025.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jeewoo1025.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-06T12:40:12+00:00</updated><id>https://jeewoo1025.github.io/feed.xml</id><title type="html">Jeewoo Sul</title><subtitle></subtitle><entry><title type="html"></title><link href="https://jeewoo1025.github.io/blog/2025/2025-04-06-attention-review/" rel="alternate" type="text/html" title=""/><published>2025-04-06T12:40:12+00:00</published><updated>2025-04-06T12:40:12+00:00</updated><id>https://jeewoo1025.github.io/blog/2025/2025-04-06-attention-review</id><content type="html" xml:base="https://jeewoo1025.github.io/blog/2025/2025-04-06-attention-review/"><![CDATA[<h2 id="Ô∏è-my-review">üíÅüèª‚Äç‚ôÄÔ∏è My Review</h2> <blockquote> <p>Paper: <a href="https://arxiv.org/abs/1706.03762v5">Attention Is All You Need</a></p> </blockquote> <p>In 2017, Google introduced the most powerful language model called <strong>Transformer</strong>. Transformer follows an encoder-decoder structure and uses residual-connected feedforward networks with attention layers in between. It‚Äôs defined by three core components as follows:</p> <ul> <li>Multi-head Attention</li> <li>Scaled Dot-Product Attention</li> <li>Positional Encoding</li> </ul> <p>‚ú® What impressed me most was how Transformer achieved a strong understanding of a context using attention mechanisms. It‚Äôs no surprise that most of today‚Äôs SOTA language models are based on Transformers.</p> <p><br/></p> <h2 id="-questions-about-transformer">üß† Questions About Transformer</h2> <h4 id="1-why-do-we-use-a-feedforward-network">1. Why do we use a Feedforward Network?</h4> <p>This is because expanding the dimensionality allows the model to perform richer computations. This helps uncover hidden patterns or features that may not be captured otherwise. It‚Äôs like giving the model more space to think.</p> <h4 id="2-why-is-it-called-a-position-wise-feedforward-network">2. Why is it called a Position-wise Feedforward Network?</h4> <p>When I looked into the code, it seemed pretty much like a regular feedforward network. ü§î So why call it ‚Äúposition-wise‚Äù?</p> <blockquote> <p>In my opinion, the authors probably wanted to emphasize that the operation is applied independently to each token (i.e., ‚Äúposition‚Äù) with its own set of weights. Even though the structure is the same, each token gets its own transformation individually.</p> </blockquote> <h4 id="3-what-are-residual-connections-and-layer-normalization-and-why-are-they-used">3. What are Residual Connections and Layer Normalization, and why are they used?</h4> <p>A residual connection adds the input <code class="language-plaintext highlighter-rouge">x</code> to the output of a layer <code class="language-plaintext highlighter-rouge">F(x)</code>, resulting in <code class="language-plaintext highlighter-rouge">x + F(x)</code>.</p> <p>‚ùì Why use it?</p> <blockquote> <p>If you take the derivative of <code class="language-plaintext highlighter-rouge">x + F(x)</code> with respect to <code class="language-plaintext highlighter-rouge">x</code>, you always get at least <code class="language-plaintext highlighter-rouge">1</code>. This helps mitigate the vanishing gradient problem, especially in deep neural networks. <br/></p> </blockquote> <p>‚ùì Why use it?</p> <blockquote> <p>Even if inputs are normalized at the input layer, as they pass through hidden layers, their distribution tends to shift. This can push activations into saturation zones (where gradients vanish). Layer normalization helps maintain a balanced distribution for effective backpropagation. üí™</p> </blockquote> <h4 id="4-what-exactly-is-positional-encoding">4. What exactly is Positional Encoding?</h4> <p>Since Transformers don‚Äôt process tokens sequentially like RNNs, they need some ways to understand token order. That‚Äôs where positional encoding comes in.</p> <p>It generates a unique position vector of shape <code class="language-plaintext highlighter-rouge">(1, d_model)</code> for each token using sine and cosine functions.</p> <p>‚ùì Why is it important?</p> <blockquote> <p>RNNs naturally process words in order, so they ‚Äúknow‚Äù where each word appears. However, Transformers process all tokens in parallel. So, positional encodings inject information about word order into the input representations.</p> </blockquote> <h4 id="5-why-do-we-use-the-attention-mechanism">5. Why do we use the Attention Mechanism?</h4> <p>Self-attention compares a token to all others in the input context to find the most relevant ones using dot products. This allows each token to ‚Äúfocus‚Äù on parts of the context that are semantically important to it.</p> <p>‚ùì Why is this valuable?</p> <blockquote> <p>This is because we can understand which parts of the input a token attends to most. As we go deeper into the encoder layers, tokens generally develop stronger attentions to contextually important words.</p> </blockquote> <p>‚ùì What are semantically similar tokens?</p> <blockquote> <p>Words that have related meanings and tend to appear in similar contexts. In vector space, they lie close to each other.</p> </blockquote> <p><br/></p> <h2 id="-recommended-pytorch-transformer-codebases">üßæ Recommended PyTorch Transformer Codebases</h2> <ul> <li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">PyTorch Official Tutorial</a></li> <li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer by Harvard NLP</a></li> <li><a href="https://github.com/paul-hyun/transformer-evolution">paul-hyun/transformer-evolution</a></li> <li><a href="https://github.com/tunz/transformer-pytorch">tunz/transformer-pytorch</a></li> </ul>]]></content><author><name></name></author></entry></feed>